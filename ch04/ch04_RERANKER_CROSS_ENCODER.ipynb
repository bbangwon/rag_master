{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e0f2a3",
   "metadata": {},
   "source": [
    "# 크로스 인코더 기반 리랭킹\n",
    "- 크로스 인코더 기반 리랭킹은 주로 BERT와 같은 인코더 기반 언어 모델을 이용해서 질문과 문서가 얼마나 잘 맞는지 관련성을 평가하는 기법입니다.\n",
    "- 질문과 문서를 하나로 결합하여 입력하고, 모델이 최종적으로 두 텍스트가 얼마나 관련 있는가를 단일 점수로 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16721ccc",
   "metadata": {},
   "source": [
    "1. 질문, 문서 입력: 질문과 문서를 \"[CLS] 질문 [SEP] 문서 [SEP]\" 형태로 결합합니다. 여기서 [CLS]는 문장의 시작을 나타내는 토큰이고 [SEP]는 두 텍스트(질문과 문서)를 구분하는 토큰입니다.\n",
    "2. 인코딩: 이렇게 결합된 문자열을 BERT와 같은 인코더 모델에 넣어, 각 토큰에 대한 임베딩 벡터를 구합니다.\n",
    "3. [CLS] 토큰 임베딩 추출: 모델의 최종 층에서 나오는 [CLS] 토큰 임베딩은 전체 입력(질문과 문서)의 맥락을 요약한 정보를 담고 있습니다. 이 임베딩은 질문과 문서 사이의 복잡한 의미적 관계를 반영합니다.\n",
    "4. 점수 계산: [CLS] 임베딩을 간단한 선형 층(fully-connected layer)이나 다른 분류 층에 통과시켜서, 최종적으로 관련성(relevance) 점수를 산출합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70979cde",
   "metadata": {},
   "source": [
    "### Bi-Encoder VS Cross-Encoder\n",
    "- 리랭킹을 다룰 때 자주 등장하는 두 가지 인코더 방식, Bi-Encoder와 Cross Encoder에 대해 살펴보겠습니다.\n",
    "#### Bi-Encoder\n",
    "- 밀집 검색에서 사용되는 인코딩 방식으로, 밀집 검색의 기반이 되는 벡터 표현을 생성합니다.\n",
    "- 질문 A와 문서 B가 있을 때, 이들 각각을 임베딩으로 동일한 크기의 벡터로 만든 뒤, 코사인 유사도와 같은 유사도 분석을 통해 벡터 간의 거리를 수치화하는 방식입니다. 두 벡터 간의 거리가 가깝다면 질문과 문서가 의미적으로 유사하다고 판단하고, 멀다면 관련성이 낮다고 판단합니다.\n",
    "\n",
    "#### Cross-Encoder\n",
    "- 질문 A와 문서 B가 있을 때, 이들을 트랜스포머 기반의 인코더 모델에 함께 투입하여, 둘 사이의 관련성을 나타내는 분류 점수를 직접 얻습니다. 이 점수가 높을수록 관련성이 높다고 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d28b4e",
   "metadata": {},
   "source": [
    "- 두 인코더 방식은 속도와 정확도에서 뚜렷한 차이를 보입니다.\n",
    "- Bi-Encoder: 각 문장을 독립적으로 인코딩합니다. 예를 들어 10만개의 문장이 있다면 10만번의 인코딩 작업만 수행하면 됩니다.\n",
    "- Cross-Encoder: 모든 가능한 문장 쌍을 동시에 인코딩합니다. 10만개의 문장이 있을 경우, 조합에 의해 약 50억개(4,999,950,000개)의 쌍을 인코딩해야 합니다.\n",
    "- 이러한 작동 방식의 차이로 인해, 대규모 데이터셋에서 Cross-Encoder는 Bi-Encoder에 비해 현저히 느린 속도를 보입니다.\n",
    "- 특히 수천 개 이상의 문장을 비교해야 하는 경우 Cross-Encoder의 속도 저하는 더욱 두드러집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c71ab",
   "metadata": {},
   "source": [
    "##### 정확도 비교\n",
    "- 일반적으로 Cross-Encoder가 Bi-Encoder에 비해 더 높은 정확도를 보입니다.\n",
    "- Bi-Encoder: 각 문장을 독립적으로 인코딩하기 때문에, 두 문장 간의 관계를 직접적으로 고려하진 않습니다. 따라서 빠른 처리가 가능하지만, 문장 간의 복잡한 상호작용을 포착하는데 한계가 있을 수 있습니다.\n",
    "- Cross-Encoder: 두 문장을 동시에 인코딩하여 하나의 임베딩을 생성합니다. 이 과정에서 두 문장 간의 관계를 직접적으로 모델링할 수 있어, 더 정확한 분류나 유사도 평가가 가능합니다.\n",
    "- 결론적으로 속도면에서는 Bi-Encoder, 정확도 측면에서는 Cross-Encoder가 유리합니다.\n",
    "- 작업의 성격과 요구사항에 따라 적절한 인코더를 선택해야 합니다.\n",
    "- 실제로는 두 방식을 조합하여 사용하기도 합니다.\n",
    "- Bi-Encoder를 사용해 대규모 데이터셋에서 후보군을 빠르게 추려낸후 Cross-Encoder를 사용해 후보군을 재평가하여 최종 순위를 결정하는 방식입니다. 빠른 속도와 정확도사이의 균형을 맞출 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e2d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = \"data/투자설명서.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "docs = loader.load_and_split(doc_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442cf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embedding = OllamaEmbeddings(model=\"bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f0091c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_store = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "persist_directory = \"data/DB\"\n",
    "faiss_store.save_local(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c9de742",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.load_local(persist_directory, embeddings=embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53022039",
   "metadata": {},
   "source": [
    "- 크로스 인코더 기반 리랭킹 알고리즘을 생성합니다.\n",
    "- ms-marco-MiniLM-L-12-v2 모델을 사용합니다.\n",
    "- Microsoft사가 개발한 MS MARCOMicrosoft MAchine Reading COmprehension 데이터셋을 기반으로 학습되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd27bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6636c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossencoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24bf4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever_with_cross_encoder(BaseRetriever):\n",
    "    vectorstore: Any = Field(description=\"초기 검색을 위한 벡터 저장소\")\n",
    "    crossencoder: Any = Field(description=\"재순위화를 위한 크로스 인코더 모델\")\n",
    "    k: int = Field(default=5, description=\"초기에 검색할 문서 수\")\n",
    "    rerank_top_k: int = Field(default=2, description=\"재순위화 후 최종적으로 반환할 문서 수\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # 초기 검색\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "\n",
    "        # 인코더용 쌍 준비\n",
    "        pairs =[[query, doc.page_content] for doc in initial_docs]\n",
    "\n",
    "        # 인코더 점수 획득\n",
    "        scores = self.crossencoder.predict(pairs)\n",
    "\n",
    "        # 점수별 문서 정렬\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 상위 재순위화 문서 반환\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae05bc",
   "metadata": {},
   "source": [
    "- Retruever_with_cross_encoder 클래스의 주요 역할은 벡터 기반 초기 검색과 크로스 인코더를 이용한 재순위화를 결합하여 더 정확한 문서 검색을 수행하는 것입니다.\n",
    "1. 초기검색: vectorstore를 사용하여 쿼리와 유사한 k개의 문서를 빠르게 검색합니다. 이는 관련 문서의 후보군을 추려내는 역할을 합니다.\n",
    "2. 문서-질문 쌍 준비: 검색된 각 문서와 질문을 쌍으로 만듭니다. 이는 크로스 인코더의 입력으로 사용됩니다.\n",
    "3. 관련성 점수 계산: 크로스 인코더 모델을 사용하여 각 질문-문서 쌍의 관련성 점수를 계산합니다.\n",
    "4. 정렬: 계산된 점수를 기준으로 문서들을 내림차순으로 정렬합니다. 이를 통해 가장 관련성 높은 문서가 상위에 오도록 합니다.\n",
    "5. 최종 결과 반환: 정렬된 문서 중 상위 rerank_top_k개의 문서만 선택하여 최종 결과로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e1f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#크로스 인코더 기반 리트리버 인스턴스 생성\n",
    "cross_encoder_retriever = Retriever_with_cross_encoder(\n",
    "    vectorstore=vectordb,\n",
    "    crossencoder=crossencoder,\n",
    "    k=4,\n",
    "    rerank_top_k=2\n",
    ")\n",
    "\n",
    "# 답변용 LLM 인스턴스 생성\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548341e",
   "metadata": {},
   "source": [
    "- vectorstore설정: 앞서 생성한 vectordb를 벡터 저장소로 지정하여 초기 밀집 검색에 사용합니다.\n",
    "- crossencoder설정: 미리 준비된 crossencoder 모델을 지정하여 검색된 문서의 재순위화에 사용합니다.\n",
    "- k값 설정: 초기 밀집 검색에서 반환할 문서의 수를 4로 설정합니다. 이는 벡터 검색을 통해 먼저 4개의 관련 문서를 추출함을 의미합니다.\n",
    "- rerank_top_k값 설정: 리랭킹 후 최종적으로 반환할 문서의 수를 2로 설정합니다. 즉, 크로스 인코더를 통해 재평가된 문서 중 가장 관련성 높은 2개만을 최종 결과로 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c67c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetruevalQA 체인 인스턴스 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=cross_encoder_retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65aeaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문: 이 회사의 2022년 영업손실이 정확히 얼마야?\n",
      "답변: 2022년 영업손실은 149.1억원입니다.\n",
      "\n",
      "답변 근거 문서:\n",
      "\\Document 1:\n",
      "하여 2021년 영업손실 130.1억원, 2022년 영업손실 149.1억원, 2023년 영업손실 122억원, 2024년\n",
      "1분기 영업손실 24.2억원이 발생하였습니다. 또한 영업 외적 측면에서도, 금융비용 등의 발생 영향\n",
      "으로 인해 2021년 당기순손실 130.7억원, 2022년 당기순손실 228.7억원, 2023년 당기순손실\n",
      "116.1억원, 2024년 1분기 당기순손실 32.9억원이 발생하는 등 지속적인 적자 구조를 면하지 못하고\n",
      "있습니다.따라서 당사의 파이프라인에서 임상 성공을 통한 기술이전, 상품화 성공 등의 성과를 이루\n",
      "\\Document 2:\n",
      "외적 측면에서도, 금융비용 등의 발생 영향으로 인해 2021년 당기순손실 130.7억원, 2022년\n",
      "당기순손실 228.7억원, 2023년 당기순손실 116.1억원, 2024년 1분기 당기순손실 32.9억원\n",
      "이 발생하는 등 지속적인 적자 구조를 면하지 못하고 있습니다. 따라서 당사의 파이프라인에\n",
      "서 임상 성공을 통한 기술이전, 상품화 성공 등의 성과를 이루어내지 못한다면 당사의 적자\n",
      "구조를 개선하는 것은 불가능할 수 있으며, 자본력이 지속적으로 감소하여 지속적인 연구개\n"
     ]
    }
   ],
   "source": [
    "query = \"이 회사의 2022년 영업손실이 정확히 얼마야?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"\\n질문: {query}\")\n",
    "print(f\"답변: {result['result']}\")\n",
    "print(\"\\n답변 근거 문서:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"\\Document {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5f75c",
   "metadata": {},
   "source": [
    "- 크로스 인코더 기반 리랭킹과 고성능 언어 모델 기반 리랭킹의 차이가 있습니다.\n",
    "- 크로스 인코더는 인코더 기반 트랜스포머를 사용하여 고성능 대규모 언어 모델에 비해 빠른 처리 속도를 제공합니다.\n",
    "- 그러나 정확도 측면에서는 고성능 언어 모델 기반에 비해 다소 떨어질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9180110",
   "metadata": {},
   "source": [
    "- 작업의 특성과 요구사항을 고려하여 빠른 처리가 필요할 경우 크로스 인코더 기반 방식, 높은 정확도가 필요할 경우 고성능 언어 모델 기반 방식이 더 적합할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
