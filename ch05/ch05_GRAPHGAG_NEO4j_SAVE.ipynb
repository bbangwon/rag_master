{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38f40d1",
   "metadata": {},
   "source": [
    "# 지식 그래프 구축\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05582cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928a91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "GRAPHRAG_FOLDER = \"./working_directory/output\"\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_AUTH = (os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f8ca1",
   "metadata": {},
   "source": [
    "- 데이터를 효율적으로 임포트하기 위한 배치 임포트 함수와 제약조건을 설정합니다.\n",
    "- 제약 조건은 DB 내 데이터 중복을 방지하기 위함입니다. Cypher문이 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cef84faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_import(statement, df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Import a dataframe into Neo4j using batched approach.\n",
    "\n",
    "    Parameters: statement is the Cypher query to execute,\n",
    "        df is the dataframe to import,\n",
    "        and batch_size is the number of rows to import in each batch.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    start_s = time.time()\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = df.iloc[start: min(start + batch_size, total)]\n",
    "        result = driver.execute_query(\n",
    "            \"UNWIND $rows AS value \" + statement,\n",
    "            rows=batch.to_dict('records'),\n",
    "            database_= NEO4J_DATABASE\n",
    "        )\n",
    "        print(result.summary.counters)\n",
    "    print(f\"{total} rows in {time.time() - start_s} s.\")\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48816c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique\n",
      "\n",
      "create constraint document_id if not exists for (d:__Document__) require d.id is unique\n",
      "\n",
      "create constraint community_id if not exists for (c:__Community__) require c.community is unique\n",
      "\n",
      "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique\n",
      "\n",
      "create constraint entity_title if not exists for (e:__Entity__) require e.name is unique\n",
      "\n",
      "create constraint covariate_title if not exists for (c:__Covariate__) require c.title is unique\n",
      "\n",
      "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\n"
     ]
    }
   ],
   "source": [
    "# 제약 조건 설정\n",
    "statements = [\n",
    "    \"\\ncreate constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique\",\n",
    "    \"\\ncreate constraint document_id if not exists for (d:__Document__) require d.id is unique\",\n",
    "    \"\\ncreate constraint community_id if not exists for (c:__Community__) require c.community is unique\",\n",
    "    \"\\ncreate constraint entity_id if not exists for (e:__Entity__) require e.id is unique\",\n",
    "    \"\\ncreate constraint entity_title if not exists for (e:__Entity__) require e.name is unique\",\n",
    "    \"\\ncreate constraint covariate_title if not exists for (c:__Covariate__) require c.title is unique\",\n",
    "    \"\\ncreate constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\",\n",
    "    \"\\n\",\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "    if len((statement or \"\").strip()) > 0:\n",
    "        print(statement)\n",
    "        driver.execute_query(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807496f3",
   "metadata": {},
   "source": [
    "### Cypher란?\n",
    "\n",
    "- Cypher는 Neo4j 그래프 데이터베이스에서 사용되는 쿼리 언어로, 그래프 데이터(노드, 관계, 속성)를 조작하고 질의하는 데 특화되어 있습니다.\n",
    "- SQL과 비슷한 선언적 구조를 가지며, MATCH(패턴 검색), CREATE(노드/관계생성), MERGE(생성 또는 업데이트), SET(속성 설정) 같은 키워드로 매칭을 통해 노드와 관계 간의 연결을 더욱 직관적으로 표현하고 탐색할 수 있다는 차이가 있습니다.\n",
    "- Neo4j Cypher 튜토리얼: https://neo4j.com/docs/getting-started/cypher/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca8385",
   "metadata": {},
   "source": [
    "- 모든 준비 과정이 끝났으니, GraphRAG의 결과물을 Neo4j Aura에 연동시키는 작업을 진행하겠습니다.\n",
    "- 최종 문서를 DB에 업로드 합니다. documents.parquet 파일에서 문서데이터를 가져와 ** Document ** 노드를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18fbe404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'properties_set': 1}\n",
      "1 rows in 0.08096933364868164 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/documents.parquet\", columns=[\"id\", \"title\"]\n",
    ")\n",
    "\n",
    "# 문서 노드 병합\n",
    "statement = \"\"\"\n",
    "MERGE (d: __Document__ {id:value.id})\n",
    "SET d += value {.title}\n",
    "\"\"\"\n",
    "batched_import(statement, doc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759f6ea",
   "metadata": {},
   "source": [
    "- 다음은 텍스트 유닛(청크)을 DB에 업로드합니다. text_units.parquet\n",
    "- ** Chunk ** 노드를 생성하고 문서와 연결합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6f2c343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'properties_set': 312}\n",
      "156 rows in 0.04469633102416992 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 유닛(청크) 임포트\n",
    "text_df = pd.read_parquet(f\"{GRAPHRAG_FOLDER}/text_units.parquet\", columns=[\"id\", \"text\", \"n_tokens\", \"document_ids\"])\n",
    "\n",
    "statement = \"\"\"\n",
    "MERGE (c: __Chunk__ {id:value.id})\n",
    "SET c += value {.text, .n_tokens}\n",
    "WITH c, value\n",
    "UNWIND value.document_ids AS document\n",
    "MATCH (d: __DOCUMENT__ {id:document})\n",
    "MERGE (c)-[:PART_OF]->(d)\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bb229",
   "metadata": {},
   "source": [
    "- 엔티티를 DB에 업로드합니다. - entities.parquet ** Entity ** 노드를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 251, 'relationships_created': 382, 'nodes_created': 251, 'properties_set': 1004}\n",
      "251 rows in 0.44866323471069336 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 엔티티 임포트\n",
    "entity_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/entities.parquet\",\n",
    "    columns=[\"title\", \"type\", \"description\", \"human_readable_id\", \"id\", \"text_unit_ids\"]\n",
    ")\n",
    "\n",
    "statement = \"\"\"\n",
    "MERGE (e: __Entity__ {id: value.id})\n",
    "SET e.human_readable_id = value.human_readable_id,\n",
    "    e.description = value.description,\n",
    "    e.name = coalesce(replace(value.title, '\"', ''), 'Unknown')\n",
    "WITH e, value\n",
    "CALL apoc.create.addLabels(e, CASE WHEN coalesce(value.type, \"\") = \"\" THEN [] ELSE [apoc.text.upperCamelCase(replace(value.type, '\"', ''))] END) YIELD node\n",
    "UNWIND value.text_unit_ids AS text_unit\n",
    "MATCH (c: __Chunk__ {id:text_unit})\n",
    "MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, entity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e466e",
   "metadata": {},
   "source": [
    "- 관계를 DB에 업로드 합니다. relationships.parquet 파일에서 관계 데이터를 로드하여 엔티티간 RELATED 관계를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37da177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'properties_set': 1348}\n",
      "337 rows in 0.06938576698303223 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 관계 임포트\n",
    "rel_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/relationships.parquet\",\n",
    "    columns=[\"source\", \"target\", \"id\", \"combined_degree\", \"weight\", \"human_readable_id\", \"description\", \"text_unit_ids\"]\n",
    ")\n",
    "\n",
    "rel_df.rename(columns={'combined_degree': 'rank'})\n",
    "\n",
    "rel_statment = \"\"\"\n",
    "MATCH (source: __Entity__ {name:replace(value.source, '\"', '')})\n",
    "MATCH (target: __Entity__ {name:replace(value.target, '\"', '')})\n",
    "MERGE (source)-[rel:RELATED {id: value.id}]->(target)\n",
    "SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}\n",
    "RETURN count(*) as createdRels\n",
    "\"\"\"\n",
    "batched_import(rel_statment, rel_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d8e21",
   "metadata": {},
   "source": [
    "- 커뮤니티를 DB에 업로드 합니다. communities.parquet 파일에서 커뮤니티 데이터를 로드하여 \\_ Community \\_\\_ 노드를 생성하고, 해당 노드에서 level과 title 속성을 설정합니다.\n",
    "- 각 커뮤니티와 관련된 텍스트 유닛들을 DB 내에서 연결하고, 커뮤니티에 속하는 엔티티들을 연결합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a75c0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 56, 'relationships_created': 614, 'nodes_created': 56, 'properties_set': 168}\n",
      "56 rows in 0.27317309379577637 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 커뮤니티 임포트\n",
    "community_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/communities.parquet\",\n",
    "    columns=[\"id\", \"level\", \"title\", \"text_unit_ids\", \"relationship_ids\"]\n",
    ")\n",
    "\n",
    "statement = \"\"\"\n",
    "MERGE (c: __Community__ {community: value.title})\n",
    "SET c.title = value.title,\n",
    "    c.level = value.level\n",
    "WITH c, value\n",
    "UNWIND value.text_unit_ids AS text_unit_id\n",
    "MATCH (t: __Chunk__ {id: text_unit_id})\n",
    "MERGE (c)-[:HAS_CHUNK]->(t)\n",
    "WITH distinct c, value\n",
    "UNWIND value.relationship_ids AS rel_id\n",
    "MATCH (start: __Entity__)-[:RELATED {id: rel_id}]->(end: __Entity__)\n",
    "MERGE (start)-[:IN_COMMUNITY]->(c)\n",
    "MERGE (end)-[:IN_COMMUNITY]->(c)\n",
    "RETURN count(distinct c) as createdCommunities\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, community_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335be604",
   "metadata": {},
   "source": [
    "- 커뮤니티 보고서를 DB에 반영합니다.\n",
    "- community_reports.parquet 파일에서 커뮤니티 보고서를 가져와 커뮤니티 노드에 속성을 추가하고, 커뮤니티의 레벨, 콘텐츠 등 속성을 업데이트합니다. 이후 발견 사항들을 개별적인 Finding 노드로 DB에 체계적으로 반영합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23a50e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'properties_set': 944}\n",
      "56 rows in 0.05147290229797363 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 커뮤니티 보고서 임포트\n",
    "community_report_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/community_reports.parquet\",\n",
    "    columns=[\"id\", \"community\", \"level\", \"title\", \"summary\", \"findings\", \"rank\", \"rating_explanation\", \"full_content\"]\n",
    ")\n",
    "\n",
    "# community 값을 \"Community \" + 숫자 형태로 문자열을 만들어줌\n",
    "community_report_df['community'] = \"Community \" + community_report_df['community'].astype(str)\n",
    "\n",
    "community_statement = \"\"\"\n",
    "MERGE (c: __Community__ {community: value.community})\n",
    "SET c.level = value.level,\n",
    "    c.name = value.title,\n",
    "    c.rank = value.rank,\n",
    "    c.rating_explanation = value.rating_explanation,\n",
    "    c.full_content = value.full_content,\n",
    "    c.summary = value.summary\n",
    "WITH c, value\n",
    "UNWIND range(0, size(value.findings) - 1) AS finding_idx\n",
    "WITH c, value, finding_idx, value.findings[finding_idx] AS finding\n",
    "MERGE (c)-[:HAS_FINDING]->(f:Finding {id: finding_idx})\n",
    "SET f += finding\n",
    "\"\"\"\n",
    "\n",
    "batched_import(community_statement, community_report_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
